# -*- coding: utf-8 -*-
"""PHASE II IMPLEMENTATION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBciT0uZJJXy92X2g89BfIa8IXglE11E

**SUMO data set**
"""

import pandas as pd

# Sample data (replace this with your actual data)
data = pd.read_csv('/content/speed11.csv')
data_df = pd.DataFrame(data)

# Set the anomaly thresholds for velocity and acceleration
velocity_threshold = 5  # Adjust this threshold as needed
acceleration_threshold = -2  # Adjust this threshold as needed

# Function to label anomalies based on thresholds
def label_anomalies(row):
    if row['Velocity'] < velocity_threshold or row['Acceleration'] < acceleration_threshold:
        return -1  # Anomaly
    else:
        return 1  # Normal

# Apply the function to create the 'Anomaly' column with ground truth values
data_df['Anomaly'] = data_df.apply(label_anomalies, axis=1)

# Save the DataFrame with the appended 'Anomaly' column back to the CSV file
data_df.to_csv('/content/speed11_with_anomaly.csv', index=False)

print(data_df)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest

# Read the data from the CSV file
data = pd.read_csv('/content/speed11_with_anomaly.csv')
data.head(15)

# Separate the data into velocity and acceleration

velocity_data = data[['Velocity']]
acceleration_data = data[['Acceleration']]

# Preprocess the data
scaler = StandardScaler()
velocity_data_scaled = scaler.fit_transform(velocity_data)
acceleration_data_scaled = scaler.fit_transform(acceleration_data)

# Combine the scaled velocity and acceleration data
combined_data = np.hstack((velocity_data_scaled, acceleration_data_scaled))

# Train the Improved Isolation Forest model
isolation_forest = IsolationForest(contamination=0.05, random_state=42)
isolation_forest.fit(combined_data)

# Predict anomalies (1 for normal data, -1 for anomalies)
anomaly_predictions = isolation_forest.predict(combined_data)

# Add the anomaly predictions to the original data
data['Anomaly'] = anomaly_predictions

# Print the data with anomaly labels
print(data)

import matplotlib.pyplot as plt
plt.figure(figsize=(6, 4))
plt.plot(data['Velocity'], label='Velocity', color='b')
plt.scatter(data[data['Anomaly'] == -1].index, data[data['Anomaly'] == -1]['Velocity'], color='r', label='Anomaly')
plt.xlabel('Data Point Index')
plt.ylabel('Velocity')
plt.title('Velocity Data with Anomalies')
plt.legend()
plt.show()

# Plot acceleration data with anomalies
plt.figure(figsize=(6, 4))
plt.plot(data['Acceleration'], label='Acceleration', color='g')
plt.scatter(data[data['Anomaly'] == -1].index, data[data['Anomaly'] == -1]['Acceleration'], color='r', label='Anomaly')
plt.xlabel('Data Point Index')
plt.ylabel('Acceleration')
plt.title('Acceleration Data with Anomalies')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Read the data from the CSV file
data = pd.read_csv('/content/speed11.csv')
data.head(15)

# Separate the data into velocity and acceleration
velocity_data = data[['Velocity']]
acceleration_data = data[['Acceleration']]

# Preprocess the data
scaler = StandardScaler()
velocity_data_scaled = scaler.fit_transform(velocity_data)
acceleration_data_scaled = scaler.fit_transform(acceleration_data)

# Combine the scaled velocity and acceleration data
combined_data = np.hstack((velocity_data_scaled, acceleration_data_scaled))

# Train the Improved Isolation Forest model
isolation_forest = IsolationForest(contamination=0.05, random_state=42)
isolation_forest.fit(combined_data)

# Select one tree from the Isolation Forest
sample_tree = isolation_forest.estimators_[0]

# Plot the selected tree
plt.figure(figsize=(6, 4))
plot_tree(sample_tree, feature_names=['Velocity', 'Acceleration'], filled=True)
plt.title("Sample Tree from Isolation Forest")
plt.show()

"""**CODE - Next Generation SIMulation vehicle trajectories and supporting DATASET**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# Load the dataset (assuming it's in a CSV file)
data = pd.read_csv("/content/Next_Generation_Simulation__NGSIM__Vehicle_Trajectories_with_Ground_Truth.csv")

# Drop any rows with missing values (NA)
data.dropna(inplace=True)

# Preprocessing and feature extraction
# Assuming we are using columns 'v_Vel', 'v_Acc', 'Global_X', 'Global_Y' as features
features = data[['v_Vel', 'v_Acc', 'Global_X', 'Global_Y']].values

# Scale the features for GMM
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Fit a Gaussian Mixture Model with a chosen number of components
#GMM
n_components = 3
gmm = GaussianMixture(n_components=n_components, random_state=42)
gmm.fit(scaled_features)

# Calculate the log-likelihood of each data point
log_probs = gmm.score_samples(scaled_features)

# Calculate the threshold for anomaly detection (adjust the threshold as needed)
threshold = np.percentile(log_probs, 5)

# Identify anomalies based on the threshold
anomalies = data[log_probs < threshold]

# Visualize the anomalies
plt.figure(figsize=(6, 4))
plt.scatter(data['v_Vel'], data['v_Acc'], c='b', label='Normal Data')
plt.scatter(anomalies['v_Vel'], anomalies['v_Acc'], c='r', label='Anomalies')
plt.xlabel('v_Vel')
plt.ylabel('v_Acc')
plt.legend()
plt.title('Anomaly Detection using Gaussian Mixture Model')
plt.show()

# Synthetic ground truth labels based on the anomaly detection results
data['Ground_Truth'] = np.where(log_probs < threshold, 1, 0)

# Confusion Matrix
from sklearn.metrics import confusion_matrix

# True labels: 0 for normal data, 1 for anomalies
true_labels = data['Ground_Truth'].values

# Predicted labels based on the anomaly detection results
predicted_labels = np.where(log_probs < threshold, 1, 0)

# Calculate the confusion matrix
confusion_mat = confusion_matrix(true_labels, predicted_labels)

# Display the confusion matrix
print("Confusion Matrix:")
print(confusion_mat)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Assuming 'Ground_Truth' column contains the true labels (0 for normal, 1 for anomalies)
true_labels = data['Ground_Truth']

# Create a binary label array (0 for normal, 1 for anomalies) based on the GMM log-likelihood threshold
predicted_labels = np.zeros(len(data))
predicted_labels[log_probs < threshold] = 1

# Create the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
classes = ['Normal', 'Anomaly']
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot(cmap='viridis', values_format='d')
plt.title('Confusion Matrix')
plt.show()

"""**1.LOCAL OUTLIER FACTOR ALGORITHM**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import LocalOutlierFactor
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Load the dataset (assuming it's in a DataFrame called 'data')
data = pd.read_csv("/content/Next_Generation_Simulation__NGSIM__Vehicle_Trajectories_with_Ground_Truth.csv")  # Replace "your_data_file.csv" with the actual file path

# Drop any rows with missing values (NA)
data.dropna(inplace=True)

# Preprocessing and feature extraction
features = data[['v_Vel', 'v_Acc']].values

# Scale the features for LOF
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Fit the Local Outlier Factor model with novelty=True
lof = LocalOutlierFactor(n_neighbors=20, contamination='auto', novelty=True)
lof.fit(scaled_features)

# Get the outlier scores for the data points (negative values indicate outliers)
outlier_scores = lof.score_samples(scaled_features)
outliers_mask = outlier_scores < np.percentile(outlier_scores, 5)

# Get the non-anomalous data points
non_anomalous_data = data[~outliers_mask]

# Apply KMeans clustering on the non-anomalous data points
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(non_anomalous_data[['v_Vel', 'v_Acc']].values)

# Assuming you have ground truth label column named 'Ground_Truth' in the DataFrame
ground_truth_labels = non_anomalous_data['Ground_Truth'].values

# Calculate the Silhouette Score using the predicted clusters and ground truth labels
silhouette_score_result1 = silhouette_score(non_anomalous_data[['v_Vel', 'v_Acc']].values, cluster_labels)
print("Silhouette Score:", silhouette_score_result1)

# Calculate anomaly scores using LOF
anomaly_scores = -outlier_scores

# Define anomaly types based on your criteria
anomaly_type1 = (data['v_Acc'] == 0) & (data['v_Vel'].diff() != 0)
anomaly_type2 = (data['v_Acc'] != 0) & (data['v_Vel'] > 25)
anomaly_type3 = data['Space_Headway'] < 30

# Combine the anomaly scores and anomaly types into a DataFrame
anomaly_data = pd.DataFrame({'Anomaly_Score': anomaly_scores,
                             'Anomaly_Type1': anomaly_type1,
                             'Anomaly_Type2': anomaly_type2,
                             'Anomaly_Type3': anomaly_type3})

# Classify anomalies based on the maximum anomaly score
# Use numpy's argmax only if the data type is compatible
if anomaly_data['Anomaly_Score'].dtype in (int, float, complex):
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values.argmax(axis=1)
else:
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].idxmax(axis=1)

# Print the classified anomalies
print(anomaly_data)

anomaly_data.to_csv('/content/anomaly_data.csv', index=True)  # Replace "/content/anomaly_data.csv"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.utils import to_categorical

# Load the anomaly data from the CSV file
anomaly_data = pd.read_csv('/content/anomaly_data.csv')  # Replace with the correct file path

# Prepare the features (X) and labels (y)
X = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values
y = anomaly_data['Anomaly_Type'].values

# Normalize the features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert the labels to one-hot encoded vectors
y = to_categorical(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the CNN model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(4, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4, activation='softmax'))
model.summary()
# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Reshape the data to fit the CNN input shape
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

import tensorflow as tf
print(tf.__version__)

from IPython.display import Image

tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')

# Show the image in Colab
Image('model.png')

"""**2.IMPROVED ISOLATION FOREST ALGORITHM**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.e etrics import silhouette_score

# Load the dataset (assuming it's in a DataFrame called 'data')
data = pd.read_csv("/content/Next_Generation_Simulation__NGSIM__Vehicle_Trajectories_with_Ground_Truth.csv")  # Replace "your_data_file.csv" with the actual file path

# Drop any rows with missing values (NA)
data.dropna(inplace=True)

# Preprocessing and feature extraction
features = data[['v_Vel', 'v_Acc']].values

# Scale the features for Isolation Forest
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Fit the Isolation Forest model
isolation_forest = IsolationForest(contamination='auto', random_state=42)
isolation_forest.fit(scaled_features)

# Get the outlier scores for the data points (negative values indicate outliers)
outlier_scores = isolation_forest.score_samples(scaled_features)
outliers_mask = outlier_scores < np.percentile(outlier_scores, 5)

# Get the non-anomalous data points
non_anomalous_data = data[~outliers_mask]

# Apply KMeans clustering on the non-anomalous data points
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(non_anomalous_data[['v_Vel', 'v_Acc']].values)

# Assuming you have ground truth label column named 'Ground_Truth' in the DataFrame
ground_truth_labels = non_anomalous_data['Ground_Truth'].values

# Calculate the Silhouette Score using the predicted clusters and ground truth labels
silhouette_score_result2 = silhouette_score(non_anomalous_data[['v_Vel', 'v_Acc']].values, cluster_labels)
print("Silhouette Score:", silhouette_score_result2)

anomaly_scores = -outlier_scores

# Define anomaly types based on your criteria
anomaly_type1 = (data['v_Acc'] == 0) & (data['v_Vel'].diff() != 0)
anomaly_type2 = (data['v_Acc'] != 0) & (data['v_Vel'] > 25)
anomaly_type3 = data['Space_Headway'] < 30

# Combine the anomaly scores and anomaly types into a DataFrame
anomaly_data = pd.DataFrame({'Anomaly_Score': anomaly_scores,
                             'Anomaly_Type1': anomaly_type1,
                             'Anomaly_Type2': anomaly_type2,
                             'Anomaly_Type3': anomaly_type3})

# Classify anomalies based on the maximum anomaly score
# Use numpy's argmax only if the data type is compatible
if anomaly_data['Anomaly_Score'].dtype in (int, float, complex):
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values.argmax(axis=1)
else:
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].idxmax(axis=1)

# Print the classified anomalies
print(anomaly_data)

anomaly_data.to_csv('/content/anomaly_data1.csv', index=True)  # Replace "/content/anomaly_data.csv"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.utils import to_categorical

# Load the anomaly data from the CSV file
anomaly_data = pd.read_csv('/content/anomaly_data1.csv')  # Replace with the correct file path

# Prepare the features (X) and labels (y)
X = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values
y = anomaly_data['Anomaly_Type'].values

# Normalize the features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert the labels to one-hot encoded vectors
y = to_categorical(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the CNN model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(4, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4, activation='softmax'))
model.summary()
# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Reshape the data to fit the CNN input shape
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from IPython.display import Image
import tensorflow as tf
tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')

# Show the image in Colab
Image('model.png')

"""**GMM**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

# Load the dataset (assuming it's in a DataFrame called 'data')
data = pd.read_csv("/content/Next_Generation_Simulation__NGSIM__Vehicle_Trajectories_with_Ground_Truth.csv")  # Replace "your_data_file.csv" with the actual file path

# Drop any rows with missing values (NA)
data.dropna(inplace=True)

# Preprocessing and feature extraction
features = data[['v_Vel', 'v_Acc']].values

# Scale the features for GMM
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Fit the Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, random_state=42)
cluster_labels = gmm.fit_predict(scaled_features)

# Calculate the Silhouette Score using the predicted clusters
silhouette_score_result = silhouette_score(scaled_features, cluster_labels)
print("Silhouette Score:", silhouette_score_result)

# Assuming you have ground truth label column named 'Ground_Truth' in the DataFrame
ground_truth_labels = data['Ground_Truth'].values

# Anomaly detection using GMM
anomaly_scores = -gmm.score_samples(scaled_features)

# Define anomaly types based on your criteria
anomaly_type1 = (data['v_Acc'] == 0) & (data['v_Vel'].diff() != 0)
anomaly_type2 = (data['v_Acc'] != 0) & (data['v_Vel'] > 25)
anomaly_type3 = data['Space_Headway'] < 30

# Combine the anomaly scores and anomaly types into a DataFrame
anomaly_data = pd.DataFrame({'Anomaly_Score': anomaly_scores,
                             'Anomaly_Type1': anomaly_type1,
                             'Anomaly_Type2': anomaly_type2,
                             'Anomaly_Type3': anomaly_type3})

# Classify anomalies based on the maximum anomaly score
# Use numpy's argmax only if the data type is compatible
if anomaly_data['Anomaly_Score'].dtype in (int, float, complex):
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values.argmax(axis=1)
else:
    anomaly_data['Anomaly_Type'] = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].idxmax(axis=1)

# Print the classified anomalies
print(anomaly_data)

anomaly_data.to_csv('/content/anomaly_data2.csv', index=True)  # Replace "/content/anomaly_data.csv"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.utils import to_categorical

# Load the anomaly data from the CSV file
anomaly_data = pd.read_csv('/content/anomaly_data2.csv')  # Replace with the correct file path

# Prepare the features (X) and labels (y)
X = anomaly_data[['Anomaly_Score', 'Anomaly_Type1', 'Anomaly_Type2', 'Anomaly_Type3']].values
y = anomaly_data['Anomaly_Type'].values

# Normalize the features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert the labels to one-hot encoded vectors
y = to_categorical(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the CNN model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(4, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4, activation='softmax'))
model.summary()
# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Reshape the data to fit the CNN input shape
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from IPython.display import Image
import tensorflow as tf
tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')

# Show the image in Colab
Image('model.png')

# results_df = pd.DataFrame({'Algorithm': algorithm_names, 'Silhouette Score': silhouette_scores})

# # Display the results table
# print(results_df)